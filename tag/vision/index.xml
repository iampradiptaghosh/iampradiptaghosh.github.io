<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vision | Pradipta Ghosh</title>
    <link>https://iampradiptaghosh.github.io/tag/vision/</link>
      <atom:link href="https://iampradiptaghosh.github.io/tag/vision/index.xml" rel="self" type="application/rss+xml" />
    <description>vision</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>2020</copyright><lastBuildDate>Sun, 05 Jul 2020 10:51:59 -0700</lastBuildDate>
    <image>
      <url>https://iampradiptaghosh.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>vision</title>
      <link>https://iampradiptaghosh.github.io/tag/vision/</link>
    </image>
    
    <item>
      <title>Sensing in Smart Cities</title>
      <link>https://iampradiptaghosh.github.io/project/smartcity/</link>
      <pubDate>Sun, 05 Jul 2020 10:51:59 -0700</pubDate>
      <guid>https://iampradiptaghosh.github.io/project/smartcity/</guid>
      <description>&lt;p&gt;This ongoing research can be categorized into three categories: Sensing Smart City Resources (e.g., public cameras), Network Synthesis, and Cross Camera Activity Detection.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sensing Smart City Resources:&lt;/strong&gt; This  category of the research focuses on automatically identifying and characterizing a wide range of sensors (cameras, microphones) in a Large City that we can leverage for many applications such as search and rescue missions, personalized fine-grain navigation, and autonomous driving assistance.
&lt;img src=&#34;https://iampradiptaghosh.github.io/img/sensecam.png&#34; alt=&#34;smartcity&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Network Synthesis:&lt;/strong&gt; The second part of the research focus on optimal usage of these public resources in smart city applications by forming an optimized network with them. For example, assume that we need an area to be monitored constantly during a search and rescue mission. In that case, we might have either a lot of unnecessary cameras or might have less than the required number of cameras already present. Building upon modern constrained optimization techniques, we are working on developing techniques that can subselect the optimal set of sensors as well as inform us if more sensors need to be deployed. If more sensors are required to be deployed, the method also outputs what are the best locations to place them.
&lt;img src=&#34;https://iampradiptaghosh.github.io/img/synthesis.png&#34; alt=&#34;synthesis&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross Camera Activity Detection:&lt;/strong&gt; The third part of research focus on detecting complex activities in smart cities. A complex activity can be “a person getting off an car then smoking then talking to another person and walking together”. While a human can easily interpret these actions, for a machine it is very hard to automatically detect such activity. In addition, if these actions occur across multiple cameras i.e., some part of the activity in one camera and the other in a different camera, then it is even a tougher problem. We have developed a system that can perform cross camera activity detection.
&lt;img src=&#34;https://iampradiptaghosh.github.io/img/caeser.gif&#34; alt=&#34;caeser&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
